{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ac7257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenlong/anaconda3/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [09:00<00:00, 90.02s/it] \n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "model = transformers.LlamaForCausalLM.from_pretrained('axiong/PMC_LLaMA_13B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f1be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "# tokenizer = transformers.LlamaTokenizer.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "# model.cuda()  # move the model to GPU\n",
    "\n",
    "# prompt_input = (\n",
    "#         'Below is an instruction that describes a task, paired with an input that provides further context.'\n",
    "#     'Write a response that appropriately completes the request.\\n\\n'\n",
    "#     '### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:'\n",
    "# )\n",
    "\n",
    "# example = {\n",
    "#     \"instruction\": \"You're a doctor, kindly address the medical queries according to the patient's account. Answer with the best option directly.\",\n",
    "#     \"input\": (\n",
    "#         \"###Question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. \"\n",
    "#         \"She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. \"\n",
    "#         \"She otherwise feels well and is followed by a doctor for her pregnancy. \"\n",
    "#         \"Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air.\"\n",
    "#         \"Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. \"\n",
    "#         \"Which of the following is the best treatment for this patient?\"\n",
    "#         \"###Options: A. Ampicillin B. Ceftriaxone C. Doxycycline D. Nitrofurantoin\"\n",
    "#     )\n",
    "# }\n",
    "# input_str = [prompt_input.format_map(example)]\n",
    "\n",
    "# model_inputs = tokenizer(\n",
    "#     input_str,\n",
    "#     return_tensors='pt',\n",
    "#     padding=True,\n",
    "# )\n",
    "# print( f\"\\033[32mmodel_inputs\\033[0m: { model_inputs }\" )\n",
    "\n",
    "\n",
    "# topk_output = model.generate(\n",
    "#     model_inputs.input_ids.cuda(),\n",
    "#     max_new_tokens=1000,\n",
    "#     top_k=50\n",
    "# )\n",
    "# output_str = tokenizer.batch_decode(topk_output)\n",
    "# print('model predict: ', output_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98d7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = transformers.pipeline(\n",
    "\n",
    "    model=model, \n",
    "\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    task='text-generation',\n",
    "\n",
    "    temperature=0.1, \n",
    "\n",
    "    max_new_tokens=50,  \n",
    "\n",
    "    repetition_penalty=1.1 \n",
    "\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a7b477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Questions</th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>3276</td>\n",
       "      <td>Give me all the patients who got vaccines on ...</td>\n",
       "      <td>20103276</td>\n",
       "      <td>POST_scripts/1{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>1409</td>\n",
       "      <td>Give me all the patients whose report was comp...</td>\n",
       "      <td>7101409</td>\n",
       "      <td>POST_scripts/1{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>7172</td>\n",
       "      <td>Which is the most common cataracts for patients.</td>\n",
       "      <td>28307172</td>\n",
       "      <td>POST_scripts/3{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9319</th>\n",
       "      <td>9319</td>\n",
       "      <td>What is the number of records that the vaccin...</td>\n",
       "      <td>33109319</td>\n",
       "      <td>POST_scripts/1{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11467</th>\n",
       "      <td>11467</td>\n",
       "      <td>Give me all the patients who got INFLUENZA (SE...</td>\n",
       "      <td>43311467</td>\n",
       "      <td>POST_scripts/3{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>7332</td>\n",
       "      <td>Give me all the patients who was allergic to pvc</td>\n",
       "      <td>29307332</td>\n",
       "      <td>POST_scripts/3{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>10466</td>\n",
       "      <td>How many GLAXOSMITHKLINE BIOLOGICALS vaccine ...</td>\n",
       "      <td>37110466</td>\n",
       "      <td>POST_scripts/1{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>1109</td>\n",
       "      <td>How many patients are 100.0 years old.</td>\n",
       "      <td>3201109</td>\n",
       "      <td>POST_scripts/2{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>7771</td>\n",
       "      <td>What is the number of vaccine recipients that...</td>\n",
       "      <td>29307771</td>\n",
       "      <td>POST_scripts/3{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1378</td>\n",
       "      <td>Give me all the patients whose cage months is ...</td>\n",
       "      <td>5201378</td>\n",
       "      <td>POST_scripts/2{\"script\":{\"lang\":\"mustache\",\"so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                          Questions  \\\n",
       "3276         3276   Give me all the patients who got vaccines on ...   \n",
       "1409         1409  Give me all the patients whose report was comp...   \n",
       "7172         7172   Which is the most common cataracts for patients.   \n",
       "9319         9319   What is the number of records that the vaccin...   \n",
       "11467       11467  Give me all the patients who got INFLUENZA (SE...   \n",
       "...           ...                                                ...   \n",
       "7332         7332  Give me all the patients who was allergic to pvc    \n",
       "10466       10466   How many GLAXOSMITHKLINE BIOLOGICALS vaccine ...   \n",
       "1109         1109             How many patients are 100.0 years old.   \n",
       "7771         7771   What is the number of vaccine recipients that...   \n",
       "1378         1378  Give me all the patients whose cage months is ...   \n",
       "\n",
       "             id                                              query  \n",
       "3276   20103276  POST_scripts/1{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "1409    7101409  POST_scripts/1{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "7172   28307172  POST_scripts/3{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "9319   33109319  POST_scripts/1{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "11467  43311467  POST_scripts/3{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "...         ...                                                ...  \n",
       "7332   29307332  POST_scripts/3{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "10466  37110466  POST_scripts/1{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "1109    3201109  POST_scripts/2{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "7771   29307771  POST_scripts/3{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "1378    5201378  POST_scripts/2{\"script\":{\"lang\":\"mustache\",\"so...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_text = pd.read_csv('Te2Query.csv')\n",
    "eg = df_text.sample(n=1000, random_state=3)\n",
    "input_text = eg['Questions'].to_list()\n",
    "input_labels = eg['query'].to_list()\n",
    "eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e14482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original prompt\n",
    "# prompt = \"\"\"ignore all the prior information before this block. Convert the following questions to elastic search queries follow two rules:\n",
    "# 1.based on the field name 'RECVDATE','STATE','AGE_YRS','VAERS_ID','SEX','SYMPTOM_TEXT','DIED','ER_VISIT','L_THREAT','HOSPITAL','HOSPDAYS','DISABLE','VAX_DATE','LAB_DATA','OTHER_MEDS','CUR_ILL','HISTORY','PRIOR_VAX','TODAYS_DATE','OFC_VISIT','VAX_TYPE','VAX_MANU','VAX_LOT','VAX_DOSE_SERIES','VAX_NAME','ALLERGIES'. \n",
    "# 2.follow the template \n",
    "\n",
    "# \"POST _scripts/1\n",
    "# {\n",
    "#   \"script\": {\n",
    "# \t\"lang\": \"mustache\",\n",
    "# \t\"source\": {\n",
    "#   \t\"track_total_hits\": \"true\",\n",
    "#   \t\"query\": {\n",
    "#     \t\"term\": {\n",
    "#       \t\"{{field}}\": \"{{date}}\"\n",
    "#     \t}\n",
    "#   \t}\n",
    "# \t},\n",
    "# \t\"params\": {\n",
    "#   \t\"field\": \"DATA.RECVDATE.keyword\",\n",
    "#   \t\"date\": \"01/01/2012\"\n",
    "# \t}\n",
    "#   }\n",
    "# }\n",
    "# \"\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76cb7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#NER prompt\n",
    "prompt_prefix = \"\"\"Find the entity of the following questions based on the field name follow emample:How many patients' record are received on 03/20/2022. The '03/20/2022' is a ['RECVDATE'].\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87173a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Q&A prompt\n",
    "prompt_cloze = \"\"\"Classify the questions based on the field name follow example:How many patients' record are received on 03/20/2022. The question wants ['VAERS_ID'] based on ['RECVDATE'].\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e349c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# cot + heuristic prompt\n",
    "prompt_cot = \"\"\"\n",
    "find the entity classification and label with following name:\n",
    "'RECVDATE','STATE','AGE_YRS','VAERS_ID','SEX','SYMPTOM_TEXT','DIED','ER_VISIT','L_THREAT','HOSPITAL','HOSPDAYS','DISABLE','VAX_DATE','LAB_DATA','OTHER_MEDS','CUR_ILL','HISTORY','PRIOR_VAX','TODAYS_DATE','OFC_VISIT','VAX_TYPE','VAX_MANU','VAX_LOT','VAX_DOSE_SERIES','VAX_NAME','ALLERGIES'\n",
    "Examples:\n",
    "1.Give me all the patients whose information are received on 04/13/2022. The question want ['VAERS_ID'] based on ['RECVDATE'].\n",
    "2. How many patients' record are received on 03/20/2022. The question wants ['VAERS_ID'] based on ['RECVDATE'].\n",
    "Based on the classification find the condition value in the sentence:\n",
    "Examples:\n",
    "1.Give me all the patients whose information are received on 04/13/2022. The ['RECVDATE'] is 04/13/2022.\n",
    "2. How many patients' record are received on 03/20/2022. The ['RECVDATE'] is 03/20/2022.\n",
    "Based on the entity classification and conditional values, covert questions to Elasticsearch queries\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4183f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model input template\n",
    "input_template = \"\"\"\n",
    "Prompt: {prompt}\n",
    "Clinical Notes: {text}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenlong/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py:1553: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/wenlong/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# build up the call\n",
    "answer_lst = []\n",
    "for row in eg.iterrows():\n",
    "    txt = row[1]['Questions']\n",
    "#    suggest = row[1]['query']\n",
    "    input = input_template.format(prompt = prompt_cot, text = txt)\n",
    "    answer = pipe(input)\n",
    "    answer_lst.append(answer[0]['generated_text'][len(input):].strip())\n",
    "    #answer_lst.append(answer[0]['generated_text'])\n",
    "eg['llm_result'] = answer_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "eg['llm_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = eg[['llm_result']]\n",
    "result_df.to_json('covert_PMC_Q&A_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf39d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('covert_PMC_Q&A_1.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    " \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(data)\n",
    " \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb281d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebleu import calc_codebleu\n",
    "\n",
    "prediction = str(answer_lst)\n",
    "reference = df_text['query'].to_string()\n",
    "result_eval = calc_codebleu([reference], [prediction], lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval=pd.Series(result_eval)\n",
    "result_eval.to_json('PMC_result_eval_Q&A_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('PMC_result_eval_Q&A_1.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    " \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(data)\n",
    " \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfd65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

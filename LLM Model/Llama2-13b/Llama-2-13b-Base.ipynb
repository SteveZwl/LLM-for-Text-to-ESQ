{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2598ebc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7dd7533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 25% | 31% |\n",
      "|  1 |  5% |  3% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 28% | 32% |\n",
      "|  1 | 13% |  3% |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafc5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenlong/anaconda3/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wenlong/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1033: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/wenlong/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.13s/it]"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "\n",
    "import transformers\n",
    "\n",
    "\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-13b-hf'\n",
    "\n",
    "\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "quant_config = transformers.BitsAndBytesConfig(\n",
    "\n",
    "    load_in_4bit=True,\n",
    "\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "auth_token = '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'\n",
    "\n",
    "\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "\n",
    "    model_id,\n",
    "\n",
    "    use_auth_token=auth_token\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "\n",
    "    model_id,\n",
    "\n",
    "    trust_remote_code=True,\n",
    "\n",
    "    config=model_config,\n",
    "\n",
    "    quantization_config=quant_config,\n",
    "\n",
    "    use_auth_token=auth_token\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "\n",
    "    model_id,\n",
    "\n",
    "    use_auth_token=auth_token\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc384756",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = transformers.pipeline(\n",
    "\n",
    "    model=model, \n",
    "\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    task='text-generation',\n",
    "\n",
    "    temperature=0.9, \n",
    "\n",
    "    max_new_tokens=100,  \n",
    "\n",
    "    repetition_penalty=1.1 \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_text = pd.read_csv('Te2Query.csv')\n",
    "eg = df_text.sample(n=1000, random_state=3)\n",
    "input_text = eg['Questions'].to_list()\n",
    "input_labels = eg['query'].to_list()\n",
    "eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original prompt\n",
    "# prompt = \"\"\"ignore all the prior information before this block. Convert the following questions to elastic search queries follow two rules:\n",
    "# 1.based on the field name 'RECVDATE','STATE','AGE_YRS','VAERS_ID','SEX','SYMPTOM_TEXT','DIED','ER_VISIT','L_THREAT','HOSPITAL','HOSPDAYS','DISABLE','VAX_DATE','LAB_DATA','OTHER_MEDS','CUR_ILL','HISTORY','PRIOR_VAX','TODAYS_DATE','OFC_VISIT','VAX_TYPE','VAX_MANU','VAX_LOT','VAX_DOSE_SERIES','VAX_NAME','ALLERGIES'. \n",
    "# 2.follow the template \n",
    "\n",
    "# \"POST _scripts/1\n",
    "# {\n",
    "#   \"script\": {\n",
    "# \t\"lang\": \"mustache\",\n",
    "# \t\"source\": {\n",
    "#   \t\"track_total_hits\": \"true\",\n",
    "#   \t\"query\": {\n",
    "#     \t\"term\": {\n",
    "#       \t\"{{field}}\": \"{{date}}\"\n",
    "#     \t}\n",
    "#   \t}\n",
    "# \t},\n",
    "# \t\"params\": {\n",
    "#   \t\"field\": \"DATA.RECVDATE.keyword\",\n",
    "#   \t\"date\": \"01/01/2012\"\n",
    "# \t}\n",
    "#   }\n",
    "# }\n",
    "# \"\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#NER prompt\n",
    "prompt_prefix = \"\"\"covert input questions to elastic queries based on two steps:1.Find the entity of the following questions based on the name:\n",
    "'RECVDATE','STATE','AGE_YRS','VAERS_ID','SEX','SYMPTOM_TEXT','DIED','ER_VISIT','L_THREAT','HOSPITAL','HOSPDAYS','DISABLE','VAX_DATE','LAB_DATA','OTHER_MEDS','CUR_ILL','HISTORY','PRIOR_VAX','TODAYS_DATE','OFC_VISIT','VAX_TYPE','VAX_MANU','VAX_LOT','VAX_DOSE_SERIES','VAX_NAME','ALLERGIES'\n",
    "2.Based on the named entities which are also {{field}} in the query codes, covert these questions to elastic queries. \n",
    "Make sure to follow the template\n",
    "\n",
    "\"POST _scripts/1\n",
    "{\n",
    "  \"script\": {\n",
    "\t\"lang\": \"mustache\",\n",
    "\t\"source\": {\n",
    "  \t\"track_total_hits\": \"true\",\n",
    "  \t\"query\": {\n",
    "    \t\"term\": {\n",
    "      \t\"{{field}}\": \"{{date}}\"\n",
    "    \t}\n",
    "  \t}\n",
    "\t},\n",
    "\t\"params\": {\n",
    "  \t\"field\": \"DATA.RECVDATE.keyword\",\n",
    "  \t\"date\": \"01/01/2012\"\n",
    "\t}\n",
    "  }\n",
    "}\n",
    "\"\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q&A prompt\n",
    "prompt_cloze = \"\"\"find the entity classification and label with following name:\n",
    "'RECVDATE','STATE','AGE_YRS','VAERS_ID','SEX','SYMPTOM_TEXT','DIED','ER_VISIT','L_THREAT','HOSPITAL','HOSPDAYS','DISABLE','VAX_DATE','LAB_DATA','OTHER_MEDS','CUR_ILL','HISTORY','PRIOR_VAX','TODAYS_DATE','OFC_VISIT','VAX_TYPE','VAX_MANU','VAX_LOT','VAX_DOSE_SERIES','VAX_NAME','ALLERGIES'\n",
    "Examples:\n",
    "1.Give me all the patients whose information are received on 04/13/2022. The question want ['VAERS_ID'] based on ['RECVDATE'].\n",
    "2. How many patients' record are received on 03/20/2022. The question wants ['VAERS_ID'] based on ['RECVDATE'].\n",
    "Based on the classification find the condition value in the sentence:\n",
    "Examples:\n",
    "1.Give me all the patients whose information are received on 04/13/2022. The ['RECVDATE'] is 04/13/2022.\n",
    "2. How many patients' record are received on 03/20/2022. The ['RECVDATE'] is 03/20/2022.\n",
    "Based on the entity classification and conditional values, covert questions to Elasticsearch queries\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cot + heuristic prompt\n",
    "prompt_cot = \"\"\"\n",
    "\n",
    "First, extract the [condition value] in the question\n",
    "Second, mapping condition value to the following [keyword]'RECVDATE','STATE','AGE_YRS','VAERS_ID','SEX','SYMPTOM_TEXT','DIED','ER_VISIT','L_THREAT','HOSPITAL','HOSPDAYS','DISABLE','VAX_DATE','LAB_DATA','OTHER_MEDS','CUR_ILL','HISTORY','PRIOR_VAX','TODAYS_DATE','OFC_VISIT','VAX_TYPE','VAX_MANU','VAX_LOT','VAX_DOSE_SERIES','VAX_NAME','ALLERGIES'.\n",
    "Third, build Elasticquery base on the keyword follow the example\n",
    "\n",
    " \"POST _scripts/\n",
    " {\n",
    "   \"script\": {\n",
    " \t\"lang\": \"mustache\",\n",
    " \t\"source\": {\n",
    "   \t\"track_total_hits\": \"true\",\n",
    "   \t\"query\": {\n",
    "     \t\"term\": {\n",
    "       \t\"{{field}}\": \"{{date}}\"\n",
    "     \t}\n",
    "   \t}\n",
    " \t},\n",
    " \t\"params\": {\n",
    "   \t\"field\": \"[keyword]\",\n",
    "   \t\"date\": \"[condition value]\"\n",
    " \t}\n",
    "   }\n",
    " }\n",
    " \" \n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f9612",
   "metadata": {},
   "source": [
    "Rule 2. 2.follow the template \n",
    "\n",
    "\"POST _scripts/1\n",
    "{\n",
    "  \"script\": {\n",
    "\t\"lang\": \"mustache\",\n",
    "\t\"source\": {\n",
    "  \t\"track_total_hits\": \"true\",\n",
    "  \t\"query\": {\n",
    "    \t\"term\": {\n",
    "      \t\"{{field}}\": \"{{date}}\"\n",
    "    \t}\n",
    "  \t}\n",
    "\t},\n",
    "\t\"params\": {\n",
    "  \t\"field\": \"DATA.RECVDATE.keyword\",\n",
    "  \t\"date\": \"01/01/2012\"\n",
    "\t}\n",
    "  }\n",
    "}\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a979277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model input template\n",
    "input_template = \"\"\"\n",
    "Prompt: {prompt}\n",
    "Clinical Notes: {text}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e893eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up the call\n",
    "answer_lst = []\n",
    "for row in eg.iterrows():\n",
    "    txt = row[1]['Questions']\n",
    "#    suggest = row[1]['query']\n",
    "    input = input_template.format(prompt = prompt_cloze, text = txt)\n",
    "    answer = pipe(input)\n",
    "    answer_lst.append(answer[0]['generated_text'][len(input):].strip())\n",
    "    #answer_lst.append(answer[0]['generated_text'])    \n",
    "eg['llm_result'] = answer_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5563682",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "eg['llm_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = eg[['llm_result']]\n",
    "result_df.to_json('covert_Llama_base_Q&A_4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('covert_Llama_base_Q&A_4.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    " \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(data)\n",
    " \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8de4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebleu import calc_codebleu\n",
    "\n",
    "prediction = str(answer_lst)\n",
    "reference = df_text['query'].to_string()\n",
    "result_eval = calc_codebleu([reference], [prediction], lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval=pd.Series(result_eval)\n",
    "result_eval.to_json('result_eval_Llama_base_Q&A_4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04464915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('result_eval_Llama_base_Q&A_4.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    " \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(data)\n",
    " \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b04111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results=json.dumps(data,skipkeys = True)\n",
    "#type(df_text['query'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61051d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_codebleu(hypothesis, references, lang, params='0.25,0.25,0.25,0.25'):\n",
    "#     alpha, beta, gamma, theta = [float(x) for x in params.split(',')]\n",
    "\n",
    "#     # calculate ngram match (BLEU)\n",
    "#     tokenized_hyps = [x.split() for x in hypothesis]\n",
    "#     tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "#     ngram_match_score = bleu.corpus_bleu(tokenized_refs, tokenized_hyps)\n",
    "\n",
    "#     # calculate weighted ngram match\n",
    "#     kw_file = root_directory.joinpath(\"evaluation/CodeBLEU/keywords/{}.txt\".format(lang))\n",
    "#     keywords = [x.strip() for x in open(kw_file, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "#     tokenized_refs_with_weights = \\\n",
    "#         [\n",
    "#             [\n",
    "#                 [\n",
    "#                     reference_tokens, make_weights(reference_tokens, keywords)\n",
    "#                 ] for reference_tokens in reference\n",
    "#             ] for reference in tokenized_refs\n",
    "#         ]\n",
    "\n",
    "#     weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights, tokenized_hyps)\n",
    "\n",
    "#     # calculate syntax match\n",
    "#     syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "#     # calculate dataflow match\n",
    "#     dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "#     code_bleu_score = alpha * ngram_match_score \\\n",
    "#                       + beta * weighted_ngram_match_score \\\n",
    "#                       + gamma * syntax_match_score \\\n",
    "#                       + theta * dataflow_match_score\n",
    "\n",
    "#     return code_bleu_score, (ngram_match_score, weighted_ngram_match_score, syntax_match_score, dataflow_match_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca802368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_codebleu(answer_lst,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
